
⏰ Lab time is 10 min
---

## About this lab
In this lab, you will run the Yugabyte TPCC benchmark. The benchmark is for Online Transaction Processing (OLTP) workloads. As such, it uses the YSQL API. The lab consists of these activities:
* Download and install the Yugabyte TPCC benchmark
* Load the data for the benchmark
* Run the benchmark
* Review the benchmark results
* Experiment on your own


### About the TPC-C benchmark
This TPC-C benchmark is from [TPC.org](https://www.tpc.org/). The YugabyteDB version of closely follows the TPC-C v5.11.0 specification. The aim of the [TPC-C](http://tpc.org/tpcc/detail5.asp?) benchmark is to test how a database performs when handling transactions generated by a real-world OLTP application. To this end, the benchmark measures **transactions per minute (tpmC)**.  The resulting TPM-C value is useful for comparing cluster topologies including geography, hardware, number of nodes, replication factor, and encryption in transit with TLS.

The data model for the TPC-C benchmark is for a business that has at least one warehouse with an inventory of items, multiple districts to serve, and various orders that consists of several items. The benchmark represents five specific transaction types:

* The **New Order** transaction simulates entering a new order through a single database transaction. To satisfy online users, this transaction, which forms the backbone of the workload, has a high frequency of execution with low latency requirements.  About 1% of these transactions simulate failures, requiring the rollback of the transaction.
* The **Payment** transaction updates the customer’s balance and reflects the payment on the district and warehouse sales statistics. This transaction includes non-primary key access to the Customer table.
* The **Order Status** transaction queries the status of a customer’s last order.
* The **Delivery** transaction processes a batch of new orders which are not yet delivered, executed in the background using a queuing mechanism.
* The **Stock Level** transaction determines the number of recently sold items that have a stock level below a specified threshold and therefore would need to be restocked.

> **Note**: The **number of warehouses** is a key configurable parameter for scale of the benchmark. When you increase the number of warehouses, you increase the data set size, the number of concurrent clients, and the number of concurrently running transactions. The reason for the increase is that a warehouse represents up the number **Point Of Sale (POS)** or **Point of Inquiry** terminals.  Terminals generate a variety of transactions such as a new order, a payment settlement, or an order status inquiry. A warehouse also represents behind the scenes database activities such as restock searches and delivery confirmations.

While the benchmark portrays the activity of a wholesale supplier, TPC-C is not limited to the activity of any particular business segment, but, rather represents any business in an industry that must manage, sell, or distribute a product or service. 


### About the Yugabyte TPCC benchmark
The [Yugabyte TPCC benchmark](https://github.com/yugabyte/tpcc) application is a fork of the popular [OLTPBench](https://github.com/oltpbenchmark/oltpbench) benchmark tool.

Just like the OLTPBench original, the Yugabyte TPCC benchmark is a multi-threaded load generator that is be able to produce a variety of workloads, including variations in rate and transaction type. The benchmark also allows for benchmark data collection. You can analyze this data to determine key metrics such as Transactions per Second (TPS) and Latency per Transaction Type.  **TPMC** remains as the main metric for summarizing the benchmark.

To run the TPCC benchmark, you use a utility script, `./tpccbenchmark`, which supports the following arguments:

```
-c,--config <arg>            [required] Workload configuration file
   --clear <arg>             Clear all records in the database for this
                             benchmark
   --create <arg>            Initialize the database for this benchmark
   --execute <arg>           Execute the benchmark workload
-h,--help                    Print this help
   --histograms              Print txn histograms
   --load <arg>              Load data using the benchmark's data loader
-o,--output <arg>            Output file (default System.out)
   --runscript <arg>         Run an SQL script
-s,--sample <arg>            Sampling window
-v,--verbose                 Display Messages
```



The default benchmark values are:
*  warehouses = 10
*  total warehouses across shards = 10
*  terminals = 100
*  dbConnections = 10
*  loaderThreads = 10
  

A [`config/workload_all.xml`](https://github.com/yugabyte/tpcc/blob/master/config/workload_all.xml) file provides an example of how to describe and configure a workload. 

For more configurations, review the forked OLTP benchmark [config.xml](https://github.com/oltpbenchmark/oltpbench/blob/master/config/sample_tpcc_config.xml).

The Yugabyte TPCC benchmark also supports multi-region cluster topologies row-level geo-partitioning. To see how, review the [`geopartitioned_workload.xml`](https://github.com/yugabyte/tpcc/blob/master/config/geopartitioned_workload.xml) file which illustrates how to specify tablespaces with specific placement policies.



### Requirements
In order to complete this lab, you'll need the following:
* Your AWS Account
* An EC2 instance running Centos 7 or similar in your VPC with Java 1.8 installed (open JDK is fine)
* A Yugabyte Universe managed by Yugabyte Platform in your VPC


## Download and install the Yugabyte TPCC benchmark

Follow these steps to download and install the benchmark:
* Using your `.pem` key, secure shell into your EC2 host where you will install and run the TPCC benchmark.

```bash
PUBLIC_IPV4_EC2_TPCC = '<PUBLIC_IPV4_EC2>'
MY_PEM_FILE= '<kp-NAME-aws.pem>'

ssh -i ~/.ssh/${MY_PEM_FILE} centos@${PUBLIC_IPV4_EC2_TPCC}

```

* Once connected, install wget and Java, if needed.
  
```bash
sudo yum install -y wget java
```

* Download and uncompress the `tpcc.targ.gz` file.
  
```bash
cd tmp/
wget https://github.com/yugabyte/tpcc/releases/download/2.0/tpcc.tar.gz
tar -zxvf tpcc.tar.gz
cd tpcc/
```

* Create a copy of the `config/workload_all.xml` file.

```bash
pwd
MY_TPCC_WORKLOAD_FILE = 'my_workload_all.xml'
cp config/workload_all.xml config/${MY_TPCC_WORKLOAD_FILE}
```

* Using VIM, edit your file.

```bash
vim config/${MY_TPCC_WORKLOAD_FILE}
```  

* Edit the file and specify values for your username and password `[i = Insert mode ]`.
  
```bash
<username>yugabyte</username>
<password>my_password</password>
```

* Save your changes `[ esc = Read-only mode  |  :wq! = force save quit ]` and exit.

* Verify you modifications.
  
```bash
cat config/${MY_TPCC_WORKLOAD_FILE}
```  


  

## Load the data for the benchmark

Before starting your benchmark workload, you first need to create the TPCC data model and then load data. Here are the steps:

* First, specify the Private IPv4 addresses for your nodes, changing the `172.21.11.x values as needed to the Private IPv4 values of the nodes your Yugabyte cluster.
  
```bash
PRIVATE_IPV4_YB_NODE_1 = '172.21.11.1'
PRIVATE_IPV4_YB_NODE_2 = '172.21.11.2'
PRIVATE_IPV4_YB_NODE_3 = '172.21.11.3'
```  

* Create the TPCC data model using the `--config`, `--create`, and `--nodes` arguments.

```bash

./tpccbenchmark --config=${MY_TPCC_WORKLOAD_FILE} --create=true  --nodes=${PRIVATE_IPV4_YB_NODE_1},${PRIVATE_IPV4_YB_NODE_2},${PRIVATE_IPV4_YB_NODE_3}
```

* Load the data for the TPCC database using the `--config`, `--load`, `--nodes`,  `--warehouses`, and `--loaderthreads` arguments.

```bash
 ./tpccbenchmark --config=${MY_TPCC_WORKLOAD_FILE} \
   --load=true  \
   --nodes=${PRIVATE_IPV4_YB_NODE_1},${PRIVATE_IPV4_YB_NODE_2},${PRIVATE_IPV4_YB_NODE_3}  \
   --warehouses=10 \
   --loaderthreads=12
```

Note: Depending on the vCPU of the nodes in your cluster and scale factor, the load time may be more than 10 minutes. The value of `--loaderthreads` typically represents the total number vCPU in your cluster. Tor example, `--loaderthreads` is 12 for a 3 node cluster with 4 vCPU per node.


## Run the benchmark

After you've loaded the data for your benchmark, you can now run the benchmark. Here are the steps:

* Using the  `--config`, `--execute`, `--nodes`, `--warehouses`, and `--historgrams` arguments, run the benchmark:

```bash
 ./tpccbenchmark  --config=${MY_TPCC_WORKLOAD_FILE} \
   --execute=true  \
   --nodes=${PRIVATE_IPV4_YB_NODE_1},${PRIVATE_IPV4_YB_NODE_2},${PRIVATE_IPV4_YB_NODE_3}  \
   --warehouses=10 \
   --histograms
```

* Verify the benchmark is running.
  
```
13:51:09,918 (DBWorkload.java:204) INFO  - Configuration -> nodes: [172.21.11.1, 172.21.11.2, 172.21.11.3], port: 5433, startWH: 1, warehouses: 10, total warehouses across shards: 10, terminals: 100, dbConnections: 10, loaderThreads: 10
[main] INFO com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
[main] INFO com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
[main] INFO com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Starting...
[main] INFO com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Start completed.
[main] INFO com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Starting...
[main] INFO com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Start completed.
13:51:35,705 (DBWorkload.java:234) INFO  - ======================================================================

{
    Benchmark = TPCC {BenchmarkModule}
    Configuration = config/workload_all.xml
    Driver = org.postgresql.Driver
    URL = [172.21.11.1, 172.21.11.2, 172.21.11.3]
    Isolation = TRANSACTION_REPEATABLE_READ
    Scale Factor = 10
}

13:51:35,705 (DBWorkload.java:235) INFO  - ======================================================================
13:51:35,722 (DBWorkload.java:468) INFO  - Creating 100 virtual terminals...
13:51:35,796 (DBWorkload.java:474) INFO  - Launching the TPCC Benchmark with 1 Phase...
13:51:35,809 (ThreadBench.java:310) INFO  - PHASE START :: [Workload=TPCC] [Serial=false] [Time=1800] [WarmupTime=0] [Rate=10000] [Arrival=REGULAR] [Ratios=[45.0, 43.0, 4.0, 4.0, 4.0]] [ActiveWorkers=100]
13:51:35,810 (ThreadBench.java:456) INFO  - MEASURE :: Warmup complete, starting measurements.
14:21:35,772 (ThreadBench.java:414) INFO  - TERMINATE :: Waiting for all terminals to finish ..
14:22:20,981 (ThreadBench.java:473) INFO  - Attempting to stop worker threads and collect measurements
14:22:20,987 (ThreadBench.java:224) INFO  - Starting WatchDogThread
14:22:21,011 (DBWorkload.java:511) INFO  - ======================================================================
14:22:21,014 (DBWorkload.java:522) INFO  -
```


## Review the benchmark results
When the benchmark completes, you will be able to review the results in the terminal.

* When the benchmark completes, review the Results, including the **TPM-C** number, **Efficiency**, and **Throughput**. Also review the Latency and Work Task Latencies results. 
  
```
14:22:21,014 (DBWorkload.java:522) INFO  -
================RESULTS================
             TPM-C |             126.73
        Efficiency |             98.55%
Throughput (req/s) |               4.75

14:22:21,036 (DBWorkload.java:689) INFO  -
======================LATENCIES (INCLUDE RETRY ATTEMPTS)=====================
 Transaction |  Count   | Avg. Latency | P99 Latency | Connection Acq Latency
    NewOrder |     3802 |        19.19 |       48.66 |                   2.89
     Payment |     3742 |        11.98 |       29.36 |                   0.95
 OrderStatus |      326 |         6.29 |       25.85 |                   1.57
    Delivery |      343 |        63.33 |      184.60 |                   1.78
  StockLevel |      338 |        20.83 |       85.70 |                   0.24
        All  |     8551 |        17.38 |       95.43 |                   1.84

14:22:21,074 (DBWorkload.java:633) INFO  -
=======================WORKER TASK LATENCIES=======================
 Transaction |     Task     |  Count   | Avg. Latency | P99 Latency
    NewOrder |   Fetch Work |     3802 |         0.07 |        3.79
    NewOrder |       Keying |     3802 |     18003.74 |    18005.02
    NewOrder |Op With Retry |     3802 |        22.31 |      228.44
    NewOrder |     Thinking |     3802 |     11945.93 |    57887.91
     Payment |   Fetch Work |     3776 |         0.06 |        1.27
     Payment |       Keying |     3776 |      3003.73 |     3005.03
     Payment |Op With Retry |     3776 |        14.06 |       85.91
     Payment |     Thinking |     3776 |     11837.72 |    55752.92
 OrderStatus |   Fetch Work |      326 |         0.06 |        0.04
 OrderStatus |       Keying |      326 |      2003.78 |     2005.04
 OrderStatus |Op With Retry |      326 |         7.91 |       26.15
 OrderStatus |     Thinking |      326 |      9702.81 |    52510.19
    Delivery |   Fetch Work |      343 |         0.07 |        2.45
    Delivery |       Keying |      343 |      2003.80 |     2005.08
    Delivery |Op With Retry |      343 |        65.14 |      278.49
    Delivery |     Thinking |      343 |      4411.94 |    19129.31
  StockLevel |   Fetch Work |      338 |         0.02 |        0.03
  StockLevel |       Keying |      338 |      2003.65 |     2005.02
  StockLevel |Op With Retry |      338 |        21.09 |       85.76
  StockLevel |     Thinking |      338 |      5186.73 |    24646.16
        All  |   Fetch Work |     8585 |         0.06 |        2.45
        All  |       Keying |     8585 |      9529.42 |    18004.98
        All  |Op with Retry |     8585 |        19.80 |      145.40
        All  |     Thinking |     8585 |     11246.03 |    55505.03
        All  |          All |     8585 |     20795.31 |    68765.58
```


 In addition to the terminal output, there are two files that your can also review: `output.json` and `results/oltpbench.csv`. The  `output.json` file contains the results of the benchmark is JSON format.
  

* Review the `output.json` file.

```bash
cat results/json/output.json
```


The `results/oltpbench.csv` file contains the transaction details such as:
* Transaction Name
* Start Time (nanoseconds)
* Connection Latency (microseconds)
* OperationLatency (microseconds)


* Review the `oltpbench.csv` file.

```bash
cat results/oltpbench.csv
```


## Experiment on your own

Now that you have a baseline benchmark, it is time to experiment. Here a some ideas:

*  In Yugabyte Platform, for your universe with your Yugabyte cluster, view the dashboard and metrics while the benchmark runs.
*  Modify your `config/workload_all.xml`, for example, changing `<useKeyingTime>` and 
    `<useThinkTime>` to `false`.
*  Create a Yugabyte cluster with TLS/SSL enabled for node to node communications and run the previous benchmark using a SSL cert.
*  Double the number of warehouses and run the same benchmark.
*  Double the nodes in the cluster and run the previous benchmark.
*  Spin up a multi-region cluster, implement tablespaces, and run the same benchmark.


## Clean up
When you are done running your TPCC benchmarks, you will want to complete the following tasks:
* Terminate your EC2 instance for running the benchmarks
* Resize your cluster back to just three nodes with a replication factor of 3
* If this is your last lab in this course, terminate all of your EC2 instances in AWS.


## Reflection

