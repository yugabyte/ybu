<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-type" content="text/html;charset=UTF-8" />
    <script
      type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    ></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({"tex2jax": {"inlineMath": [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({"HTML-CSS": {"availableFonts":["TeX"],"scale": 150}});
    </script>

    <style>
      /*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

      body {
        font-family: "Segoe WPC", "Segoe UI", "SFUIText-Light",
          "HelveticaNeue-Light", sans-serif, "Droid Sans Fallback";
        font-size: 14px;
        padding: 0 12px;
        line-height: 22px;
        word-wrap: break-word;
      }

      #code-csp-warning {
        position: fixed;
        top: 0;
        right: 0;
        color: white;
        margin: 16px;
        text-align: center;
        font-size: 12px;
        font-family: sans-serif;
        background-color: #444444;
        cursor: pointer;
        padding: 6px;
        box-shadow: 1px 1px 1px rgba(0, 0, 0, 0.25);
      }

      #code-csp-warning:hover {
        text-decoration: none;
        background-color: #007acc;
        box-shadow: 2px 2px 2px rgba(0, 0, 0, 0.25);
      }

      body.scrollBeyondLastLine {
        margin-bottom: calc(100vh - 22px);
      }

      body.showEditorSelection .code-line {
        position: relative;
      }

      body.showEditorSelection .code-active-line:before,
      body.showEditorSelection .code-line:hover:before {
        content: "";
        display: block;
        position: absolute;
        top: 0;
        left: -12px;
        height: 100%;
      }

      body.showEditorSelection li.code-active-line:before,
      body.showEditorSelection li.code-line:hover:before {
        left: -30px;
      }

      .vscode-light.showEditorSelection .code-active-line:before {
        border-left: 3px solid rgba(0, 0, 0, 0.15);
      }

      .vscode-light.showEditorSelection .code-line:hover:before {
        border-left: 3px solid rgba(0, 0, 0, 0.4);
      }

      .vscode-dark.showEditorSelection .code-active-line:before {
        border-left: 3px solid rgba(255, 255, 255, 0.4);
      }

      .vscode-dark.showEditorSelection .code-line:hover:before {
        border-left: 3px solid rgba(255, 255, 255, 0.6);
      }

      .vscode-high-contrast.showEditorSelection .code-active-line:before {
        border-left: 3px solid rgba(255, 160, 0, 0.7);
      }

      .vscode-high-contrast.showEditorSelection .code-line:hover:before {
        border-left: 3px solid rgba(255, 160, 0, 1);
      }

      img {
        max-width: 100%;
        max-height: 100%;
      }

      a {
        color: #4080d0;
        text-decoration: none;
      }

      a:focus,
      input:focus,
      select:focus,
      textarea:focus {
        outline: 1px solid -webkit-focus-ring-color;
        outline-offset: -1px;
      }

      hr {
        border: 0;
        height: 2px;
        border-bottom: 2px solid;
      }

      h1 {
        padding-bottom: 0.3em;
        line-height: 1.2;
        border-bottom-width: 1px;
        border-bottom-style: solid;
      }

      h1,
      h2,
      h3 {
        font-weight: normal;
      }

      h1 code,
      h2 code,
      h3 code,
      h4 code,
      h5 code,
      h6 code {
        font-size: inherit;
        line-height: auto;
      }

      a:hover {
        color: #4080d0;
        text-decoration: underline;
      }

      table {
        border-collapse: collapse;
      }

      table > thead > tr > th {
        text-align: left;
        border-bottom: 1px solid;
      }

      table > thead > tr > th,
      table > thead > tr > td,
      table > tbody > tr > th,
      table > tbody > tr > td {
        padding: 5px 10px;
      }

      table > tbody > tr + tr > td {
        border-top: 1px solid;
      }

      blockquote {
        margin: 0 7px 0 5px;
        padding: 0 16px 0 10px;
        border-left: 5px solid;
      }

      code {
        font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New",
          monospace, "Droid Sans Fallback";
        font-size: 14px;
        line-height: 19px;
      }

      body.wordWrap pre {
        white-space: pre-wrap;
      }

      .mac code {
        font-size: 12px;
        line-height: 18px;
      }

      pre:not(.hljs),
      pre.hljs code > div {
        padding: 16px;
        border-radius: 3px;
        overflow: auto;
      }

      /** Theming */

      .vscode-light,
      .vscode-light pre code {
        color: rgb(30, 30, 30);
      }

      .vscode-dark,
      .vscode-dark pre code {
        color: #ddd;
      }

      .vscode-high-contrast,
      .vscode-high-contrast pre code {
        color: white;
      }

      .vscode-light code {
        color: #a31515;
      }

      .vscode-dark code {
        color: #d7ba7d;
      }

      .vscode-light pre:not(.hljs),
      .vscode-light code > div {
        background-color: rgba(220, 220, 220, 0.4);
      }

      .vscode-dark pre:not(.hljs),
      .vscode-dark code > div {
        background-color: rgba(10, 10, 10, 0.4);
      }

      .vscode-high-contrast pre:not(.hljs),
      .vscode-high-contrast code > div {
        background-color: rgb(0, 0, 0);
      }

      .vscode-high-contrast h1 {
        border-color: rgb(0, 0, 0);
      }

      .vscode-light table > thead > tr > th {
        border-color: rgba(0, 0, 0, 0.69);
      }

      .vscode-dark table > thead > tr > th {
        border-color: rgba(255, 255, 255, 0.69);
      }

      .vscode-light h1,
      .vscode-light hr,
      .vscode-light table > tbody > tr + tr > td {
        border-color: rgba(0, 0, 0, 0.18);
      }

      .vscode-dark h1,
      .vscode-dark hr,
      .vscode-dark table > tbody > tr + tr > td {
        border-color: rgba(255, 255, 255, 0.18);
      }

      .vscode-light blockquote,
      .vscode-dark blockquote {
        background: rgba(127, 127, 127, 0.1);
        border-color: rgba(0, 122, 204, 0.5);
      }

      .vscode-high-contrast blockquote {
        background: transparent;
        border-color: #fff;
      }
    </style>

    <style>
      /* Tomorrow Theme */
      /* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
      /* Original theme - https://github.com/chriskempson/tomorrow-theme */

      /* Tomorrow Comment */
      .hljs-comment,
      .hljs-quote {
        color: #8e908c;
      }

      /* Tomorrow Red */
      .hljs-variable,
      .hljs-template-variable,
      .hljs-tag,
      .hljs-name,
      .hljs-selector-id,
      .hljs-selector-class,
      .hljs-regexp,
      .hljs-deletion {
        color: #c82829;
      }

      /* Tomorrow Orange */
      .hljs-number,
      .hljs-built_in,
      .hljs-builtin-name,
      .hljs-literal,
      .hljs-type,
      .hljs-params,
      .hljs-meta,
      .hljs-link {
        color: #f5871f;
      }

      /* Tomorrow Yellow */
      .hljs-attribute {
        color: #eab700;
      }

      /* Tomorrow Green */
      .hljs-string,
      .hljs-symbol,
      .hljs-bullet,
      .hljs-addition {
        color: #718c00;
      }

      /* Tomorrow Blue */
      .hljs-title,
      .hljs-section {
        color: #4271ae;
      }

      /* Tomorrow Purple */
      .hljs-keyword,
      .hljs-selector-tag {
        color: #8959a8;
      }

      .hljs {
        display: block;
        overflow-x: auto;
        color: #4d4d4c;
        padding: 0.5em;
      }

      .hljs-emphasis {
        font-style: italic;
      }

      .hljs-strong {
        font-weight: bold;
      }
    </style>

    <style>
      /*
 * Markdown PDF CSS
 */

      pre {
        background-color: #f8f8f8;
        border: 1px solid #cccccc;
        border-radius: 3px;
        overflow-x: auto;
        white-space: pre-wrap;
        overflow-wrap: break-word;
      }

      pre:not(.hljs) {
        padding: 23px;
        line-height: 19px;
      }

      blockquote {
        background: rgba(127, 127, 127, 0.1);
        border-color: rgba(0, 122, 204, 0.5);
      }

      .emoji {
        height: 1.4em;
      }

      /* for inline code */
      :not(pre):not(.hljs) > code {
        color: #c9ae75; /* Change the old color so it seems less like an error */
        font-size: inherit;
      }
    </style>
  </head>
  <body>
    <h1 id="run-a-workload-on-a-multi-node-universe">
      Run a Workload on a Multi-node Universe
    </h1>
    <h2 id="introduction">Introduction</h2>
    <p>
      In this hands-on lab, you will use xCluster replication to connect two
      independent YugabyteDB universes in a multi-region topology. The reason
      why a client would want this type of topology is to create high
      availability and resiliency in different geographical regions.
    </p>
    <p>
      Up until now, you have created a single cluster that is geographically
      distributed across different availability zones. You can also stretch a
      cluster across different geographic regions as well. These topologies are
      considered synchronous replication.
    </p>
    <p>
      But many clients often can't justify the additional complexity or
      operational costs involved with a synchronous replication in a
      multi-regional topology. These clients can use the asynchronous
      replication offered by xCluster to save and simplify, although the trade
      off is a decrease in consistency from transactional to timeline
      consistency due to the asynchronous nature of the replication.
    </p>
    <p>
      For a deeper dive into the different types of regional replication
      methodologies and their use cases, review the
      <a
        href="https://docs.yugabyte.com/latest/explore/multi-region-deployments/"
        >Yugabyte docs on multi-region deployment</a
      >.
    </p>
    <p>
      In this lab, you will connect two independent three node multi-zone
      clusters. You will use AWS as the cloud provider to deploy both clusters.
      One cluster will be in located in us-west-2 and the other will be in the
      us-east-1. You will begin by implementing unidirectional replication,
      meaning a source cluster will replicate its data to a target cluster. Any
      writes made to the source cluster will be replicated to the target
      cluster. Any writes made to the target cluster however, will not be
      replicated to the source cluster. Hence the unidirectional data flow only
      goes from the source to the target, in our case the west to the east. This
      is known as an Active-Passive set up.
    </p>
    <p>
      After that, you will implement a bidirectional replication, meaning both
      clusters will replicate data changes to the other cluster region. This is
      known as an Active-Active replication.
    </p>
    <p>
      It is important to note that with asynchronous replication, there is no
      longer a dependency on the Raft consensus algorithm. Instead, the
      YugabyteDB layer uses the change data capture (CDC) tool to ensure that
      changes in the data are automatically applied to remote data repositories.
      Similar to the Raft consensus, CDC is applied at the DocDB layer and
      therefore works with both YSQL and YCQL APIs. For a closer look at the
      <a
        href="https://docs.yugabyte.com/latest/architecture/docdb-replication/change-data-capture/"
        >CDC technology take a look at the Yugabyte Docs.</a
      >
    </p>
    <blockquote>
      <p>
        <strong>Important:</strong> Although the Raft Consensus synchronous
        replication method guarantees transactional consistency, CDC
        asynchronous replication can only offer timeline consistency.
        <a
          href="https://en.wikipedia.org/wiki/Consistency_model#Consistency_and_replication"
          >For a deeper dive on the different types of consistency models in
          relation to replication view the following article on Wikipedia.</a
        >
      </p>
    </blockquote>
    <h3 id="objective">Objective</h3>
    <p>
      As a sales engineer, I want to connect two YugabyteDB clusters located in
      different regions to demonstrate xCluster replication.
    </p>
    <h2 id="requirements">Requirements</h2>
    <ul>
      <li>
        <p>
          A deployed YugabyteDB cluster on AWS in us-east-1 on Platform version
          2.11.2.
        </p>
      </li>
      <li>
        <p>
          A deployed YugabyteDB cluster on AWS in us-west-2 on Platform version
          2.11.2..
        </p>
      </li>
      <li>
        <p>
          Both <code>.pem</code> key files to connect of their respective
          YugabyteDB servers hosting Platform.
        </p>
      </li>
      <li>
        <p>Yugabyte Platform credentials</p>
      </li>
    </ul>
    <h2 id="cluster-setup">Cluster setup</h2>
    <p>
      In the initial set up for both clusters, there are a couple things to keep
      in mind.
    </p>
    <p>
      First, it's recommended to use the same versions of Platform to avoid any
      configuration errors. This lab demonstrates xCluster replication with two
      cluster on Platform version 2.11.2. To get a detailed step by step lab on
      how to upgrade your Platform version, take a look at
      <strong>LAB: Platform Version Update</strong>
    </p>
    <p>
      Second, you must change the length of time the YB-Master will declare a
      node unhealthy since this topology requires remote network communication.
      This is done with a G-Flag called
      <strong>--leader_failure_max_missed_heartbeat_periods</strong>. We will
      extend this time from the default value of 6 seconds to 10 seconds. You
      can do this in the Platform UI by following the step by step directions in
      <strong>LAB: Rolling Config Changes</strong>.
    </p>
    <h2 id="create-a-peering-connection">Create a Peering Connection</h2>
    <p>
      To connect two regions together in AWS, a peering connection is needed to
      link the regional VPCs together. This route will allow a cluster to
      replicate data to the other cluster.
    </p>
    <p>
      A virtual private cloud or VPC is an isolated virtual network located in a
      region. For more information about
      <a
        href="https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html"
        >how VPCs work, visit the AWS docs.</a
      >
    </p>
    <h3 id="create-the-peering-connection-request">
      Create the peering connection request
    </h3>
    <p>
      To create a peering connection you must assign requester and accepter
      roles to each VPC. In this lab, you will assign the VPC in the west to
      become the requester and the VPC in the east region to be the accepter.
      This designation is actually arbitrary and can easily be reversed, but
      should be designated in order to avoid confusion, especially if more VPCs
      need to be connected. For a closer look at
      <a
        href="https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html"
        >VPC peering visit the AWS docs.</a
      >
    </p>
    <p>To create the peering connection request following the instructions:</p>
    <ul>
      <li>
        <p>
          Navigate to the AWS VPC console in the us-west-2 region in the
          browser.
        </p>
      </li>
      <li>
        <p>
          Make a note of the VPC id for the requester VPC in the us-west-2
          region. You will need this later to accept the peer connection.
        </p>
      </li>
      <li>
        <p>
          Make a note of the CIDR block for the requester VPC. This will be
          added to the routing table in the accepter VPC later in this lesson.
        </p>
      </li>
      <li>
        <p>
          Select the Peering Connections option located on the left menu in the
          Virtual Private Cloud section as shown in the following image:
        </p>
      </li>
    </ul>
    <p>
      <img
        src="file:///Users/markkim/Documents/YBU_repos/YBU_public/TFE/YBU-35/assets/images/100-vpc_west_1600x700.png"
        alt="VPC in US West (Oregon)."
      />
    </p>
    <ul>
      <li>
        <p>
          Select the <strong>Create peering connection</strong> button on the
          top right side of the page.
        </p>
      </li>
      <li>
        <p>
          Name the peering connection using the standard naming convention,
          pc-&lt;4-letter-username&gt;-usw2-use1, to state the regions that will
          be connected, for example, <strong>pc-CKIM-usw2-use1</strong>.
        </p>
      </li>
      <li>
        <p>
          Select the local VPC in that region that will be connected, in this
          case, vpc-CKIM-usw2.
        </p>
      </li>
      <li>
        <p>
          Once the VPC is selected, note that the CIDR block is automatically
          populated.
        </p>
      </li>
    </ul>
    <blockquote>
      <p>
        <strong>Important:</strong> In order for the peering connections to
        work, the CIDR blocks cannot overlap. This would cause a conflict and
        make the peering connection fail.
      </p>
    </blockquote>
    <ul>
      <li>
        <p>
          Select the region that contains the VPC you wish to connect to, in
          this case <strong>us-east-1</strong>.
        </p>
      </li>
      <li>
        <p>
          Enter the VPC ID of the accepter VPC, located in the
          <strong>us-east-1</strong> region.
        </p>
      </li>
      <li>
        <p>
          Add the Name tag, pc-CKIM-usw2-use1, to identify the regions being
          connected.
        </p>
      </li>
      <li>
        <p>
          Select <strong>Create peering connection</strong> to provision this
          network connection.
        </p>
      </li>
      <li>
        <p>
          If successful, a message states that the peering connection request
          has been created, however to make this connection active, this request
          must be accepted.
        </p>
      </li>
    </ul>
    <h3 id="accept-the-peering-connection-request">
      Accept the peering connection request
    </h3>
    <ul>
      <li>
        <p>Navigate to the us-east-1 VPC console in the browser.</p>
      </li>
      <li>
        <p>
          Make a note of the VPC id for the accepter VPC in the us-east-1
          region. You will need this later to validate the peer connection.
        </p>
      </li>
      <li>
        <p>
          Make a note of the CIDR block for the accepter VPC. This will be added
          to the routing table in the requester VPC later in this lesson.
        </p>
      </li>
      <li>
        <p>
          Select the <strong>Peering Connections</strong> option in the menu on
          the left side of the page.
        </p>
      </li>
      <li>
        <p>
          Select the radio button of the peering connection that is in the state
          <strong>Pending acceptance</strong>. The peering connection can
          further be validated by comparing the VPC IDs of the two VPCs being
          connected.
        </p>
      </li>
      <li>
        <p>Select the Action drop down at the top right of the page.</p>
      </li>
      <li>
        <p>Select <strong>Accept request</strong>.</p>
      </li>
      <li>
        <p>
          Verify the VPC IDs are correct and select
          <strong>Accept request</strong>.
        </p>
      </li>
    </ul>
    <p>
      Now that the peering connection has been created, one last step is needed
      to ensure this network connection has been made. You need to add the CIDR
      block to the routing table of each VPC. This ensures that requests will be
      routed to the correct address.
    </p>
    <h3 id="add-the-peering-connection-routes">
      Add the peering connection routes
    </h3>
    <p>
      Now that the peering connection has been established, you must add the
      final step to route the requests to the correct address. You will do this
      by adding a route to the routing table in the respective VPC. By adding
      the respective CIDR block of each VPC, you can enable bidirectional
      communication between the two clusters.
    </p>
    <ul>
      <li>
        <p>
          In the us-east-1 VPC console, select <strong>Your VPCs</strong> to
          view all the VPCs in this region.
        </p>
      </li>
      <li>
        <p>
          Select the accepter VPC, in this case vpc-SLUE-use1. This will
          navigate you to the details page of the accepter VPC.
        </p>
      </li>
      <li>
        <p>
          Select the <strong>Main route table</strong> to navigate to the Route
          tables page.
        </p>
      </li>
      <li>
        <p>Scroll down to and select the <strong>Routes</strong> tab.</p>
      </li>
      <li>
        <p>Select <strong>Edit routes</strong>.</p>
      </li>
      <li>
        <p>Select <strong>Add route</strong>.</p>
      </li>
      <li>
        <p>
          In the Destination field add the CIDR block for the requester VPC in
          the west region.
        </p>
      </li>
      <li>
        <p>
          In the Target field select <strong>Peering Connection</strong> and add
          the connection you created, for example,
          <strong>pc-MKIM-usw2-use1</strong>.
        </p>
      </li>
      <li>
        <p>Select Save changes.</p>
      </li>
      <li>
        <p>
          Go the the us-west-2 region and add the CIDR block for the accepter
          VPC for the routing table for same peering connection and save the
          changes.
        </p>
      </li>
    </ul>
    <p>
      Now the network route between the VPCs has been established, you need to
      verify that the connection work.
    </p>
    <p>
      One way to test this connection is to SSH into the EC2 instance hosting
      Platform in the west region. Then try to SSH from that EC2 instance into
      the EC2 instance in the east region.
    </p>
    <p>
      You will need to carefully copy the respective .pem key file with a
      notepad or text editor, not an IDE, to the EC2 instance in order to
      establish this connection. Remember to delete this key once this test has
      completed for security concerns.
    </p>
    <h2 id="create-xcluster-replication">Create xCluster replication</h2>
    <p>
      Now that the network provisioning work has been completed, the remaining
      tasks to create an xCluster with unidirectional replication will be
      accomplished with Yugabyte Platform.
    </p>
    <p>
      In this section here are the steps you'll follow to create a
      unidirectional replication:
    </p>
    <ul>
      <li>
        <p>Create the identical table in both clusters</p>
      </li>
      <li>
        <p>Retrieve the universe ID</p>
      </li>
      <li>
        <p>Retrieve the Master addresses</p>
      </li>
      <li>
        <p>Retrieve the table ID</p>
      </li>
      <li>
        <p>Configure the universe with the replication command</p>
      </li>
    </ul>
    <h3 id="create-the-table-to-replicate">Create the table to replicate</h3>
    <blockquote>
      <p>
        <strong>Important:</strong> First you must create a table on both
        clusters that are identical. With xCluster replication, you must
        designate which table(s) will be replicated. xCluster allows you to be
        selective about which tables are mission critical and HA.
      </p>
    </blockquote>
    <p>
      In both clusters, create a <code>keyvalue</code> table using the following
      steps:
    </p>
    <ul>
      <li>
        <p>
          SSH into the EC2 instance that hosts Yugabyte Platform. The prompt
          will change to <strong>[centos@platform ~]$</strong> if the EC2 was
          launched with centos.
        </p>
      </li>
      <li>
        <p>
          SSH into the node that contains the YB-Master leader. You can find
          this information in the Yugabyte Platform UI. Once you sign in and
          select the universe, select the <strong>Nodes</strong> tab. Then
          select the Leader, indicated as the Master (Leader) in
          <strong>Processes</strong>. Then choose <strong>Connect</strong> from
          the <strong>Actions</strong> drop down list. Execute this script in
          the EC2 instance terminal that was accessed in the previous step to be
          connected to the node that contains the YB-Master leader.
        </p>
      </li>
    </ul>
    <p>
      If successful, the prompt will change again to display the IP address and
      the user as shown in the following example:
    </p>
    <div class="highlight">
      <pre><code class="language-sql">    [yugabyte@ip-172-152-110-225 ~]$
</code></pre>
    </div>
    <ul>
      <li>Next enter the following script command to access the YSQL shell:</li>
    </ul>
    <div class="highlight">
      <pre><code class="language-sql">    /home/yugabyte/tserver/bin/ysqlsh -h &lt;my-node-IP-address&gt; -p 5433 -U yugabyte
</code></pre>
    </div>
    <p>Use the IP address of the node in the proceeding script command.</p>
    <ul>
      <li>
        Connect to the <code>postgres</code> database with the following
        command:
      </li>
    </ul>
    <div class="highlight">
      <pre><code class="language-sql">    yugabyte=<span class="hljs-comment"># \c postgres</span>
</code></pre>
    </div>
    <ul>
      <li>
        Create the <strong>keyvalue</strong> table with the following command:
      </li>
    </ul>
    <div class="highlight">
      <pre><code class="language-sql">    CREATE TABLE keyvalue (k int, v int, PRIMARY KEY (k));
</code></pre>
    </div>
    <ul>
      <li>Populate the table with data with the following command:</li>
    </ul>
    <div class="highlight">
      <pre><code class="language-sql">    INSERT INTO keyvalue VALUES (1, 2);
</code></pre>
    </div>
    <ul>
      <li>
        Verify the table and data are in the table with the following command:
      </li>
    </ul>
    <div class="highlight">
      <pre><code class="language-sql">    SELECT * FROM keyvalue;
</code></pre>
    </div>
    <p>
      If the table and data were populated successfully, you will get the
      following response in the terminal:
    </p>
    <div class="highlight">
      <pre><code class="language-sql">    k | v
---+---
 1 | 2
</code></pre>
    </div>
    <h3
      id="get-the-cluster-identifier-and-master-addresses-in-the-source-cluster"
    >
      Get the cluster identifier and master addresses in the source cluster
    </h3>
    <p>
      Now that the table has been created, you will need to retain the
      identifier id for the cluster and node's master addresses in order to
      configure the xCluster replication.
    </p>
    <ul>
      <li>
        <p>
          Navigate to the source cluster node YSQL shell terminal, and run the
          <strong>exit</strong> command. This will close the YSQL shell and
          return to the node instance terminal.
        </p>
      </li>
      <li>
        <p>
          In the terminal for the node, enter the following command to view the
          configuration for the cluster as shown in the following script
          command:
        </p>
      </li>
    </ul>
    <div class="highlight">
      <pre><code class="language-sql">    [yugabyte@ip-172-152-110-225 ~]$ cat /home/yugabyte/tserver/conf/server.conf
</code></pre>
    </div>
    <p>
      This command returns the data needed to assign xCluster replication. The
      key properties you must write down are the
      <strong>tserver_master_addrs</strong> and the
      <strong>cluster_uuid</strong> for the SOURCE cluster.
    </p>
    <div class="highlight">
      <pre><code class="language-sql">    --placement_cloud=aws
--placement_region=us-west-2
...
--tserver_master_addrs=172.151.37.55:7100,172.151.62.129:7100,172.151.78.12:7100
...
--cluster_uuid=e069287c-534b-41e8-83c8-c0659c0bacea
...
--leader_failure_max_missed_heartbeat_periods=10
</code></pre>
    </div>
    <blockquote>
      <p>
        <strong>Pro Tip:</strong> These G-flag attributes enables fine tuning of
        the YugabyteDB cluster which makes the YugabyteDB run more efficiently
        and productively to a clients' apps specific needs.
      </p>
    </blockquote>
    <p>
      Take special note that the following properties and their values in the
      proceeding response:
    </p>
    <ul>
      <li>
        <p>tserver_master_addrs</p>
      </li>
      <li>
        <p>cluster_uuid</p>
      </li>
      <li>
        <p>leader_failure_max_missed_heartbeat_periods</p>
      </li>
    </ul>
    <p>
      The <strong>tserver_master_addrs</strong> are the needed to use the
      <code>yb-admin</code> tool to enable xCluster replication. This property
      is referred to as the <strong>master_addresses</strong> in Yugabyte Docs.
      Note that these master addresses are assigned to port 7100. Here is an
      example of what the <strong>master_addresses</strong> looks like,
      172.152.110.225:7100,172.152.79.76:7100,172.152.87.11:7100 for a three
      node cluster.
    </p>
    <p>
      The <strong>cluster_uuid</strong> is the unique identifier for the
      universe. This will be used to identify the source and target universes.
      Here is an example of what a <strong>cluster_uuid</strong> looks like:
      e069287c-534b-41e8-83c8-c0659c0bacea.
    </p>
    <p>
      Note that the G-flag property,
      <strong>leader_failure_max_missed_heartbeat_periods</strong>, has the
      value of 10. You assigned this value at the beginning of the lesson in
      order to enable the master to take into account the larger latency
      involved with geographically distant communication.
    </p>
    <h3 id="get-the-table-id">Get the table id</h3>
    <p>
      In order to correctly configure cross cluster replication, you need to not
      only identify the cluster ids, but also the table ids. Note that you will
      connect one table in this lab, but multiple tables can also be configured
      for replication.
    </p>
    <p>
      To retrieve the table id, you will use the
      <strong>yb-admin</strong> utility.
    </p>
    <p>
      The <strong>yb-admin</strong> utility is a powerful command line interface
      that is used to manage the cluster's attributes. You can retrieve
      important information in much more detail than possible in the Platform UI
      by using the commands such as <strong>list all master</strong> to find out
      the master addresses as well as which node contains the leader. You can
      also get information about the cluster and if there are any current
      producers or replications currently set up on the cluster with the
      command, <strong>get_universe_config</strong>. For more Refer to the
      <a href="https://docs.yugabyte.com/latest/admin/yb-admin/"
        >Yugabyte docs to find a detailed list of commands for the yb-admin
        utility.</a
      >
    </p>
    <blockquote>
      <p>
        <strong>Important:</strong> In order the use the yb-admin utility, the
        master addresses of the nodes must be added for every command. Currently
        there is an
        <a href="https://github.com/yugabyte/yugabyte-db/issues/8844"
          >open GitHub issue</a
        >
        to disassociate this dependency.
      </p>
    </blockquote>
    <p>
      To retrieve the table id using the <strong>yb-admin</strong> utility enter
      the following script in the terminal of the leader node:
    </p>
    <div class="highlight">
      <pre><code class="language-sql">    /home/yugabyte/master/bin/yb-admin \
    -master_addresses \ &lt;master address <span class="hljs-keyword">in</span> <span class="hljs-built_in">source</span> cluster&gt; \
    list_tables include_table_id | grep postgres.keyvalue
</code></pre>
    </div>
    <p>
      Replace the master addresses in your source cluster in the statement
      above. Note that the list tables search will look for the table
      <strong>postgres.keyvalue</strong>.
    </p>
    <p>
      Now that the cluster id, master addresses, and table id have been
      retrieved for the source cluster, repeat the proceeding steps for the
      target cluster in the east region as well. You will need this information
      for the xCluster replication.
    </p>
    <h3 id="configure-unidirectional-replication">
      Configure unidirectional replication
    </h3>
    <p>
      In order to create a replication of the table, <strong>keyvalues</strong>,
      in a cross cluster asynchronous replication, you will need to configure
      the target cluster to replicate from the source cluster. You will use
      <strong>yb-admin</strong> to administer this replication pattern.
    </p>
    <p>
      In the target cluster's leader node's terminal, populate the following
      command with the information retrieved from the previous step:
    </p>
    <div class="highlight">
      <pre><code class="language-sql">    /home/yugabyte/master/bin/yb-admin \
-master_addresses \
&lt;target-master-addresses&gt; \
setup_universe_replication &lt;<span class="hljs-built_in">source</span>-universe_uuid&gt; &lt;source_master_addresses&gt; &lt;<span class="hljs-built_in">source</span>-table-ids&gt;
</code></pre>
    </div>
    <p>
      For more information regarding these attributes visit the
      <a
        href="https://docs.yugabyte.com/latest/admin/yb-admin/#setup-universe-replication"
        >Yugabyte Docs regarding setup for universe replication.</a
      >
    </p>
    <blockquote>
      <p>
        <strong>Pro tip:</strong> The IP addresses for the master addresses and
        the tserver addresses use the IP address of the nodes. The distinction
        between the master addresses and tserver addresses are the port numbers,
        7100 for masters and 9100 for the tservers respectively.
      </p>
    </blockquote>
    <h3 id="validate-xcluster-replication">Validate xCluster replication</h3>
    <p>
      To test that the xCluster replication has been set up successfully, insert
      data into the <strong>keyvalue</strong> table in the source cluster in the
      west region.
    </p>
    <ul>
      <li>
        Access the YSQL shell in the leader node and enter the following
        command:
      </li>
    </ul>
    <div class="highlight">
      <pre><code class="language-sql">    <span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> keyvalues <span class="hljs-keyword">VALUES</span> (<span class="hljs-number">3</span>, <span class="hljs-number">6</span>);
</code></pre>
    </div>
    <ul>
      <li>Verify the data has populated the table in the source cluster.</li>
    </ul>
    <div class="highlight">
      <pre><code class="language-sql">    <span class="hljs-keyword">SELECT</span> * <span class="hljs-keyword">FROM</span> keyvalues;
</code></pre>
    </div>
    <ul>
      <li>
        In another terminal window, access the TARGET cluster and open the YSQL
        shell and enter the following command:
      </li>
    </ul>
    <div class="highlight">
      <pre><code class="language-sql">    <span class="hljs-keyword">SELECT</span> * <span class="hljs-keyword">FROM</span> keyvalues;
</code></pre>
    </div>
    <p>
      The response from the table will verify that the last entry has been added
      to the <strong>keyvalue</strong> table and xCluster replication is active.
    </p>
    <h3 id="bidirectional-replication">Bidirectional replication</h3>
    <p>
      Currently if you try to create an entry in the target cluster, the data
      will not replicate into the source cluster. In order to create a
      bidirectional replication, navigate into the source cluster's leader node
      terminal and repeat the proceeding steps with the target universe. Use the
      following command as a guide to enter the universe identifier data:
    </p>
    <div class="highlight">
      <pre><code class="language-sql">    /home/yugabyte/master/bin/yb-admin \
-master_addresses \
&lt;<span class="hljs-built_in">source</span>-master-addresses&gt; \
setup_universe_replication &lt;target-universe_uuid&gt; &lt;target_master_addresses&gt; &lt;target-table-ids&gt;
</code></pre>
    </div>
    <p>
      Validate that bidirectional replication is working by logging into the
      YSQL shell in the leader node terminal and enter another row into the
      <strong>keyvalues</strong> table.
    </p>
    <h2 id="reflection">Reflection</h2>
    <p>
      Nice work, you have set up a unidirectional and bidirectional replication
      using xCluster. This is a powerful topology to create HA and resiliency in
      the data layer. You have also learned the power of the
      <strong>yb-admin</strong> utility to retrieve data and manipulate the
      universe. <strong>yb-admin</strong> can be used to change topologies,
      create backups, restore backups, and much more.
    </p>
  </body>
</html>
